{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd036e404a227338681f70667fc3ca45b89ab1565564ab5470ce06ba9279221c71d",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import init\n",
    "\n",
    "# Dataloader \n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "\n",
    "import utils\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "# Model \n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "# Config\n",
    "import configparser\n",
    "\n",
    "# Train Function\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "import logging\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "#import utils\n",
    "#from configs import Config\n",
    "#from model import GCNMultiBlock\n",
    "#from dataloader import Sign_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====encoder and decoder functions=====\n",
    "\n",
    "#to convert labels to categories (list of int)\n",
    "def labels2cat(label_encoder, list):\n",
    "    return label_encoder.transform(list)\n",
    "\n",
    "#to encode labels to one-hot vectors\n",
    "def labels2onehot(onehot_encoder, label_encoder, labels):\n",
    "    return onehot_encoder.transform(label_encoder.transform(labels).reshape(-1, 1)).toarray()\n",
    "\n",
    "#to decode one-hot to labels\n",
    "def onehot2labels(label_encoder, y_onehot):\n",
    "    return label_encoder.inverse_transform(np.where(y_onehot == 1)[1]).tolist()\n",
    "\n",
    "#to convert categories to labels\n",
    "def cat2labels(label_encoder, y_cat):\n",
    "    return label_encoder.inverse_transform(y_cat).tolist()\n",
    "\n",
    "\n",
    "# ------------------ RNN utils ------------------------##\n",
    "def init_gru(gru):\n",
    "    if isinstance(gru, nn.GRU) or isinstance(gru, nn.GRUCell):\n",
    "        for param in gru.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "\n",
    "def pad_and_pack_sequence(input_sequence):\n",
    "    # pad sequences to have same length\n",
    "    input_sequence = nn.utils.rnn.pad_sequence(input_sequence, batch_first=True)\n",
    "    # calculate lengths of sequences and **store in a tensor**, otherwise pytorch cannot trace correctly.\n",
    "    seq_lengths = torch.LongTensor(list(map(len, input_sequence)))\n",
    "    # create packed sequence        \n",
    "    packed_input_sequence = nn.utils.rnn.pack_padded_sequence(input_sequence, seq_lengths, batch_first=True,\n",
    "                                                              enforce_sorted=False)\n",
    "\n",
    "    return packed_input_sequence\n",
    "\n",
    "\n",
    "def batch_select_tail(batch, in_lengths):\n",
    "    \"\"\" Select tensors from a batch based on the time indices.\n",
    "    \n",
    "    E.g.     \n",
    "    batch = tensor([[[ 0,  1,  2,  3],\n",
    "                     [ 4,  5,  6,  7],\n",
    "                     [ 8,  9, 10, 11]],\n",
    "                     [[12, 13, 14, 15],\n",
    "                     [16, 17, 18, 19],\n",
    "                     [20, 21, 22, 23]]])\n",
    "    of size = (2, 3, 4)\n",
    "    \n",
    "    indices = tensor([1, 2])\n",
    "    \n",
    "    returns tensor([[4, 5, 6, 7],\n",
    "                    [20, 21, 22, 23]])\n",
    "    \"\"\"\n",
    "    rv = torch.stack([torch.index_select(batch[i], 0, in_lengths[i] - 1).squeeze(0) for i in range(batch.size(0))])\n",
    "\n",
    "    return rv\n",
    "\n",
    "\n",
    "def batch_mean_pooling(batch, in_lengths):\n",
    "    \"\"\" Select tensors from a batch based on the input sequence lengths. And apply mean pooling over it.\n",
    "    E.g.\n",
    "    batch = tensor([[[ 0,  1,  2,  3],\n",
    "                     [ 4,  5,  6,  7],\n",
    "                     [ 8,  9, 10, 11]],\n",
    "                     [[12, 13, 14, 15],\n",
    "                     [16, 17, 18, 19],\n",
    "                     [20, 21, 22, 23]]])\n",
    "    of size = (2, 3, 4)\n",
    "    indices = tensor([1, 2])\n",
    "    returns tensor([[0, 1, 2, 3],\n",
    "                    [14, 15, 16, 17]])\n",
    "    \"\"\"\n",
    "    mean = []\n",
    "\n",
    "    for idx, instance in enumerate(batch):\n",
    "        keep = instance[:int(in_lengths[idx])]\n",
    "\n",
    "        mean.append(torch.mean(keep, dim=0))\n",
    "\n",
    "    mean = torch.stack(mean, dim=0)\n",
    "    return mean\n",
    "\n",
    "\n",
    "def gather_last(batch_hidden_states, in_lengths):\n",
    "    num_hidden_states = int(batch_hidden_states.size(2))\n",
    "\n",
    "    indices = in_lengths.unsqueeze(1).unsqueeze(1) - 1\n",
    "    indices = indices.repeat(1, 1, num_hidden_states)\n",
    "\n",
    "    return torch.gather(batch_hidden_states, 1, indices).squeeze(1)\n",
    "\n",
    "\n",
    "# --------------- plotting utils -------------------- #\n",
    "\n",
    "def plot_curves(A=None, B=None, C=None, D=None):\n",
    "    if not A:\n",
    "        A = np.load('output/epoch_training_losses.npy')\n",
    "        B = np.load('output/epoch_training_scores.npy')\n",
    "        C = np.load('output/epoch_test_loss.npy')\n",
    "        D = np.load('output/epoch_test_score.npy')\n",
    "\n",
    "    epochs = A.shape[0]\n",
    "    # plot\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.plot(np.arange(1, epochs + 1), np.mean(A, axis=1))  # train loss (on epoch end)\n",
    "    plt.plot(np.arange(1, epochs + 1), C)  # test loss (on epoch end)\n",
    "    plt.title(\"model loss\")\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(['train', 'test'], loc=\"upper left\")\n",
    "\n",
    "    # 2nd figure\n",
    "    plt.subplot(122)\n",
    "    plt.plot(np.arange(1, epochs + 1), np.mean(B, axis=1))  # train accuracy (on epoch end)\n",
    "    plt.plot(np.arange(1, epochs + 1), D)  # test accuracy (on epoch end)\n",
    "    plt.title(\"training scores\")\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(['train', 'test'], loc=\"upper left\")\n",
    "    title = \"output/curves.png\"\n",
    "    plt.savefig(title, dpi=600)\n",
    "    # plt.close(fig)\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues,\n",
    "                          save_to=None):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    # classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label',\n",
    "           )\n",
    "\n",
    "    ax.tick_params(axis='both', which='major', labelsize=5)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if save_to:\n",
    "        plt.savefig(save_to, dpi=600)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def load_data(path=\"../data/cora/\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sign_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, file_name_index, split, pose_directory, sampling_method='random', num_samples=25, num_copies=4,\n",
    "                 img_transforms=None, video_transforms=None, test_index_file=None):\n",
    "        \n",
    "        assert os.path.exists(file_name_index), \"File path does not exist...check...: {}.\".format(file_name_index)\n",
    "        assert os.path.exists(pose_directory), \"File to pose does not exist...check... {}.\".format(pose_directory)\n",
    "\n",
    "        self.data = []\n",
    "        self.label_encoder, self.onehot_encoder = LabelEncoder(), OneHotEncoder(categories='auto')\n",
    "\n",
    "        if type(split) == 'str':\n",
    "            split = [split]\n",
    "\n",
    "        self.test_index_file = test_index_file\n",
    "        self.init_dataset(file_name_index, split)\n",
    "\n",
    "        self.file_name_index = file_name_index\n",
    "        self.pose_directory = pose_directory\n",
    "        self.framename = 'image_{}_keypoints.json'\n",
    "        self.sampling_method = sampling_method\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "        self.img_transforms = img_transforms\n",
    "        self.video_transforms = video_transforms\n",
    "\n",
    "        self.num_copies = num_copies\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        video_id, label_number, frame_start, frame_end = self.data[index]\n",
    "        # frames of dimensions (T, H, W, C)\n",
    "        x = self._load_poses(video_id, frame_start, frame_end, self.sampling_method, self.num_samples)\n",
    "\n",
    "        if self.video_transforms:\n",
    "            x = self.video_transforms(x)\n",
    "\n",
    "        y = label_number\n",
    "\n",
    "        return x, y, video_id\n",
    "\n",
    "    def init_dataset(self, file_name_index, split):\n",
    "        with open(file_name_index, 'r') as f:\n",
    "            content = json.load(f)\n",
    "\n",
    "        # create label encoder\n",
    "        labels = sorted([label_entry['gloss'] for label_entry in content])\n",
    "\n",
    "        self.label_encoder.fit(labels)\n",
    "        self.onehot_encoder.fit(self.label_encoder.transform(self.label_encoder.classes_).reshape(-1, 1))\n",
    "\n",
    "        if self.test_index_file is not None:\n",
    "            print('Trained on {}, tested on {}'.format(file_name_index, self.test_index_file))\n",
    "            with open(self.test_index_file, 'r') as f:\n",
    "                content = json.load(f)\n",
    "\n",
    "        # make dataset\n",
    "        for label_entry in content:\n",
    "            gloss, instances = label_entry['gloss'], label_entry['instances']\n",
    "            label_number = utils.labels2cat(self.label_encoder, [gloss])[0]\n",
    "\n",
    "            for instance in instances:\n",
    "                if instance['split'] not in split:\n",
    "                    continue\n",
    "\n",
    "                frame_end = instance['frame_end']\n",
    "                frame_start = instance['frame_start']\n",
    "                video_id = instance['video_id']\n",
    "\n",
    "                instance_entry = video_id, label_number, frame_start, frame_end\n",
    "                self.data.append(instance_entry)\n",
    "\n",
    "    def _load_poses(self, video_id, frame_start, frame_end, sampling_method, num_samples):\n",
    "        \"\"\" Load frames of a video. Start and end indices are provided just to avoid listing and sorting the directory unnecessarily.\n",
    "         \"\"\"\n",
    "        poses = []\n",
    "\n",
    "        if sampling_method == 'random':\n",
    "            frames_to_sample = rand_start_sampling(frame_start, frame_end, num_samples)\n",
    "        elif sampling_method == 'seq':\n",
    "            frames_to_sample = sequential_sampling(frame_start, frame_end, num_samples)\n",
    "        elif sampling_method == 'k_copies':\n",
    "            frames_to_sample = k_copies_fixed_length_sequential_sampling(frame_start, frame_end, num_samples,\n",
    "                                                                         self.num_copies)\n",
    "        \n",
    "        else:\n",
    "            raise NotImplementedError('Unimplemented sample strategy found: {}.'.format(sampling_method))\n",
    "\n",
    "        for i in frames_to_sample:\n",
    "            pose_path = os.path.join(self.pose_directory, video_id, self.framename.format(str(i).zfill(5)))\n",
    "            # pose = cv2.imread(frame_path, cv2.COLOR_BGR2RGB)\n",
    "            pose = read_pose_file(pose_path)\n",
    "\n",
    "            if pose is not None:\n",
    "                if self.img_transforms:\n",
    "                    pose = self.img_transforms(pose)\n",
    "\n",
    "                poses.append(pose)\n",
    "            else:\n",
    "                try:\n",
    "                    poses.append(poses[-1])\n",
    "                except IndexError:\n",
    "                    print(pose_path)\n",
    "\n",
    "        pad = None\n",
    "\n",
    "        # if len(frames_to_sample) < num_samples:\n",
    "        if len(poses) < num_samples:\n",
    "            num_padding = num_samples - len(frames_to_sample)\n",
    "            last_pose = poses[-1]\n",
    "            pad = last_pose.repeat(1, num_padding)\n",
    "\n",
    "        poses_across_time = torch.cat(poses, dim=1)\n",
    "        if pad is not None:\n",
    "            poses_across_time = torch.cat([poses_across_time, pad], dim=1)\n",
    "\n",
    "        return poses_across_time\n",
    "\n",
    "\n",
    "def rand_start_sampling(frame_start, frame_end, num_samples):\n",
    "    \"\"\"Randomly select a starting point and return the continuous ${num_samples} frames.\"\"\"\n",
    "    num_frames = frame_end - frame_start + 1\n",
    "\n",
    "    if num_frames > num_samples:\n",
    "        select_from = range(frame_start, frame_end - num_samples + 1)\n",
    "        sample_start = random.choice(select_from)\n",
    "        frames_to_sample = list(range(sample_start, sample_start + num_samples))\n",
    "    else:\n",
    "        frames_to_sample = list(range(frame_start, frame_end + 1))\n",
    "\n",
    "    return frames_to_sample\n",
    "\n",
    "\n",
    "def sequential_sampling(frame_start, frame_end, num_samples):\n",
    "    \"\"\"Keep sequentially ${num_samples} frames from the whole video sequence by uniformly skipping frames.\"\"\"\n",
    "    num_frames = frame_end - frame_start + 1\n",
    "\n",
    "    frames_to_sample = []\n",
    "    if num_frames > num_samples:\n",
    "        frames_skip = set()\n",
    "\n",
    "        num_skips = num_frames - num_samples\n",
    "        interval = num_frames // num_skips\n",
    "\n",
    "        for i in range(frame_start, frame_end + 1):\n",
    "            if i % interval == 0 and len(frames_skip) <= num_skips:\n",
    "                frames_skip.add(i)\n",
    "\n",
    "        for i in range(frame_start, frame_end + 1):\n",
    "            if i not in frames_skip:\n",
    "                frames_to_sample.append(i)\n",
    "    else:\n",
    "        frames_to_sample = list(range(frame_start, frame_end + 1))\n",
    "\n",
    "    return frames_to_sample\n",
    "\n",
    "def k_copies_fixed_length_sequential_sampling(frame_start, frame_end, num_samples, num_copies):\n",
    "    num_frames = frame_end - frame_start + 1\n",
    "\n",
    "    frames_to_sample = []\n",
    "\n",
    "    if num_frames <= num_samples:\n",
    "        num_pads = num_samples - num_frames\n",
    "\n",
    "        frames_to_sample = list(range(frame_start, frame_end + 1))\n",
    "        frames_to_sample.extend([frame_end] * num_pads)\n",
    "\n",
    "        frames_to_sample *= num_copies\n",
    "\n",
    "    elif num_samples * num_copies < num_frames:\n",
    "        mid = (frame_start + frame_end) // 2\n",
    "        half = num_samples * num_copies // 2\n",
    "\n",
    "        frame_start = mid - half\n",
    "\n",
    "        for i in range(num_copies):\n",
    "            frames_to_sample.extend(list(range(frame_start + i * num_samples,\n",
    "                                               frame_start + i * num_samples + num_samples)))\n",
    "\n",
    "    else:\n",
    "        stride = math.floor((num_frames - num_samples) / (num_copies - 1))\n",
    "        for i in range(num_copies):\n",
    "            frames_to_sample.extend(list(range(frame_start + i * stride,\n",
    "                                               frame_start + i * stride + num_samples)))\n",
    "\n",
    "    return frames_to_sample\n",
    "\n",
    "\n",
    "def read_pose_file(filepath):\n",
    "    \n",
    "    body_pose_exclude = {9, 10, 11, 22, 23, 24, 12, 13, 14, 19, 20, 21}\n",
    "\n",
    "    try:\n",
    "        content = json.load(open(filepath))[\"people\"][0]\n",
    "    except IndexError:\n",
    "        return None\n",
    "\n",
    "    path_parts = os.path.split(filepath)\n",
    "\n",
    "    frame_id = path_parts[1][:11]\n",
    "    vid = os.path.split(path_parts[0])[-1]\n",
    "\n",
    "    save_to = os.path.join('/home/jovyan/Documents/DL/DL_Project/WLASL/code/Pose-GCN/posegcn/features', vid)\n",
    "\n",
    "    try:\n",
    "        ft = torch.load(os.path.join(save_to, frame_id + '_ft.pt'))\n",
    "\n",
    "        xy = ft[:, :2]\n",
    "        # angles = torch.atan(ft[:, 110:]) / 90\n",
    "        # ft = torch.cat([xy, angles], dim=1)\n",
    "        return xy\n",
    "    except FileNotFoundError:\n",
    "        \n",
    "        # print(filepath)\n",
    "        body_pose = content[\"pose_keypoints_2d\"]\n",
    "        left_hand_pose = content[\"hand_left_keypoints_2d\"]\n",
    "        right_hand_pose = content[\"hand_right_keypoints_2d\"]\n",
    "\n",
    "        body_pose.extend(left_hand_pose)\n",
    "        body_pose.extend(right_hand_pose)\n",
    "\n",
    "        x = [v for i, v in enumerate(body_pose) if i % 3 == 0 and i // 3 not in body_pose_exclude]\n",
    "        y = [v for i, v in enumerate(body_pose) if i % 3 == 1 and i // 3 not in body_pose_exclude]\n",
    "        # conf = [v for i, v in enumerate(body_pose) if i % 3 == 2 and i // 3 not in body_pose_exclude]\n",
    "\n",
    "        x = 2 * ((torch.FloatTensor(x) / 256.0) - 0.5)\n",
    "        y = 2 * ((torch.FloatTensor(y) / 256.0) - 0.5)\n",
    "        # conf = torch.FloatTensor(conf)\n",
    "\n",
    "        x_diff = torch.FloatTensor(get_difference(x)) / 2\n",
    "        y_diff = torch.FloatTensor(get_difference(y)) / 2\n",
    "\n",
    "        zero_indices = (x_diff == 0).nonzero()\n",
    "\n",
    "        orient = y_diff / x_diff\n",
    "        orient[zero_indices] = 0\n",
    "\n",
    "        xy = torch.stack([x, y]).transpose_(0, 1)\n",
    "\n",
    "        ft = torch.cat([xy, x_diff, y_diff, orient], dim=1)\n",
    "\n",
    "        path_parts = os.path.split(filepath)\n",
    "\n",
    "        frame_id = path_parts[1][:11]\n",
    "        vid = os.path.split(path_parts[0])[-1]\n",
    "\n",
    "        save_to = os.path.join('/home/jovyan/Documents/DL/DL_Project/WLASL/code/Pose-GCN/posegcn/features', vid)\n",
    "        if not os.path.exists(save_to):\n",
    "            os.mkdir(save_to)\n",
    "        torch.save(ft, os.path.join(save_to, frame_id + '_ft.pt'))\n",
    "\n",
    "        xy = ft[:, :2]\n",
    "        # angles = torch.atan(ft[:, 110:]) / 90\n",
    "        # ft = torch.cat([xy, angles], dim=1)\n",
    "        #\n",
    "        return xy\n",
    "    \n",
    "\n",
    "def get_difference(x):\n",
    "    diff = []\n",
    "\n",
    "    for i, j in enumerate(x):\n",
    "        temp = []\n",
    "        for j, k in enumerate(x):\n",
    "            if i != j:\n",
    "                temp.append(j - k)\n",
    "\n",
    "        diff.append(temp)\n",
    "\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TGCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic GCN Layer \n",
    "# Reference: https://towardsdatascience.com/program-a-simple-graph-net-in-pytorch-e00b500a642d\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, acti=True):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        if acti:\n",
    "            self.acti = nn.ReLU(inplace=True)\n",
    "        else:\n",
    "            self.acti = None\n",
    "    def forward(self, F):\n",
    "        output = self.linear(F)\n",
    "        if not self.acti:\n",
    "            return output\n",
    "        return self.acti(output)\n",
    "\n",
    "# GC block module with stacked GCN and BatchNorm layers\n",
    "# BN is for standardising input to layer for each mini-batch. Helps to stablise the learning process\n",
    "class GCBlock(nn.Module):\n",
    "    def __init__(self, in_features, p_dropout, bias=True, is_resi=True):\n",
    "        super(GCBlock, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = in_features\n",
    "        # Set residual links to deal with gradient outflow\n",
    "        self.is_resi = is_resi\n",
    "\n",
    "        self.gc1 = GCNLayer(in_features, in_features)\n",
    "        self.bn1 = nn.BatchNorm1d(55 * in_features)\n",
    "\n",
    "        self.gc2 = GCNLayer(in_features, in_features)\n",
    "        self.bn2 = nn.BatchNorm1d(55 * in_features)\n",
    "\n",
    "        self.do = nn.Dropout(p_dropout)\n",
    "        self.act_f = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.gc1(x)\n",
    "        b, n, f = y.shape\n",
    "        y = self.bn1(y.view(b, -1)).view(b, n, f)\n",
    "        y = self.act_f(y)\n",
    "        y = self.do(y)\n",
    "\n",
    "        y = self.gc2(y)\n",
    "        b, n, f = y.shape\n",
    "        y = self.bn2(y.view(b, -1)).view(b, n, f)\n",
    "        y = self.act_f(y)\n",
    "        y = self.do(y)\n",
    "        # Use residual links\n",
    "        if self.is_resi:\n",
    "            return y + x\n",
    "        else:\n",
    "            return y\n",
    "\n",
    "# Multi GCBlock model with FC final layer for classification\n",
    "class GCNMultiBlock(nn.Module):\n",
    "    def __init__(self, input_feature, hidden_feature, num_class, p_dropout, num_stage=1, is_resi=True):\n",
    "        super(GCNMultiBlock, self).__init__()\n",
    "        self.num_stage = num_stage\n",
    "\n",
    "        self.gc1 = GCNLayer(input_feature, hidden_feature)\n",
    "        self.bn1 = nn.BatchNorm1d(55 * hidden_feature)\n",
    "\n",
    "        self.gcbs = []\n",
    "        for i in range(num_stage):\n",
    "            self.gcbs.append(GCBlock(hidden_feature, p_dropout=p_dropout, is_resi=is_resi))\n",
    "\n",
    "        self.gcbs = nn.ModuleList(self.gcbs)\n",
    "\n",
    "        self.do = nn.Dropout(p_dropout)\n",
    "        self.act_f = nn.Tanh()\n",
    "\n",
    "        self.fc_out = nn.Linear(hidden_feature, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.gc1(x)\n",
    "        b, n, f = y.shape\n",
    "        y = self.bn1(y.view(b, -1)).view(b, n, f)\n",
    "        y = self.act_f(y)\n",
    "        y = self.do(y)\n",
    "\n",
    "        for i in range(self.num_stage):\n",
    "            y = self.gcbs[i](y)\n",
    "\n",
    "        out = torch.mean(y, dim=1)\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, config_path):\n",
    "        config = configparser.ConfigParser()\n",
    "        config.read(config_path)\n",
    "\n",
    "        # training\n",
    "        train_config = config['TRAIN']\n",
    "        self.batch_size = int(train_config['BATCH_SIZE'])\n",
    "        self.max_epochs = int(train_config['MAX_EPOCHS'])\n",
    "        self.log_interval = int(train_config['LOG_INTERVAL'])\n",
    "        self.num_samples = int(train_config['NUM_SAMPLES'])\n",
    "        self.drop_p = float(train_config['DROP_P'])\n",
    "\n",
    "        # optimizer\n",
    "        opt_config = config['OPTIMIZER']\n",
    "        self.init_lr = float(opt_config['INIT_LR'])\n",
    "        self.adam_eps = float(opt_config['ADAM_EPS'])\n",
    "        self.adam_weight_decay = float(opt_config['ADAM_WEIGHT_DECAY'])\n",
    "\n",
    "        # GCN\n",
    "        gcn_config = config['GCN']\n",
    "        self.hidden_size = int(gcn_config['HIDDEN_SIZE'])\n",
    "        self.num_stages = int(gcn_config['NUM_STAGES'])\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'bs={}_ns={}_drop={}_lr={}_eps={}_wd={}'.format(\n",
    "            self.batch_size, self.num_samples, self.drop_p, self.init_lr, self.adam_eps, self.adam_weight_decay\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'TRAIN' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-eede78f50c7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mMAX_EPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mLOG_INTERVAL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"during training, only take NUM_SAMPLES frames per video\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TRAIN' is not defined"
     ]
    }
   ],
   "source": [
    "[TRAIN]\n",
    "BATCH_SIZE = 64\n",
    "MAX_EPOCHS = 200\n",
    "LOG_INTERVAL = 1\n",
    "; during training, only take NUM_SAMPLES frames per video\n",
    "NUM_SAMPLES = 50\n",
    "DROP_P = 0.3\n",
    "\n",
    "[OPTIMIZER]\n",
    "INIT_LR = 0.001\n",
    "ADAM_EPS = 1e-3\n",
    "ADAM_WEIGHT_DECAY = 0\n",
    "\n",
    "[GCN]\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_STAGES = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(file_split, pose_directory, configs, save_model=None):\n",
    "    \n",
    "    epochs = configs.max_epochs\n",
    "    log_interval = configs.log_interval\n",
    "    num_samples = configs.num_samples\n",
    "    hidden_size = configs.hidden_size\n",
    "    drop_p = configs.drop_p\n",
    "    num_stages = configs.num_stages\n",
    "\n",
    "    # setup dataset\n",
    "    \n",
    "    train_dataset = Sign_Dataset(file_name_index=file_split, split=['train', 'val'], pose_directory=pose_directory, img_transforms=None, video_transforms=None, num_samples=num_samples )\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=configs.batch_size,\n",
    "                                                    shuffle=True)\n",
    "\n",
    "    val_dataset = Sign_Dataset(file_name_index=file_split, split='test', pose_directory=pose_directory,\n",
    "                               img_transforms=None, video_transforms=None,\n",
    "                               num_samples=num_samples, sampling_method='k_copies')\n",
    "    \n",
    "    val_data_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=configs.batch_size,\n",
    "                                                  shuffle=True)\n",
    "\n",
    "    # setup the model\n",
    "    model = GCNMultiBlock(input_feature=num_samples*2, hidden_feature=num_samples*2,\n",
    "                         num_class=len(train_dataset.label_encoder.classes_), p_dropout=drop_p, num_stage=num_stages).cuda()\n",
    "\n",
    "    # setup training parameters, learning rate, optimizer, scheduler\n",
    "    lr = configs.init_lr\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, eps=configs.adam_eps, weight_decay=configs.adam_weight_decay)\n",
    "\n",
    "\n",
    "    best_test_acc = 0\n",
    "    # start training\n",
    "    for epoch in range(int(epochs)):\n",
    "        # train, test model\n",
    "\n",
    "        print('start training.')\n",
    "        train_losses, train_scores, train_gts, train_preds = train(log_interval, model,\n",
    "                                                                   train_data_loader, optimizer, epoch)\n",
    "        print('start testing.')\n",
    "        val_loss, val_score, val_gts, val_preds, incorrect_samples = validation(model,\n",
    "                                                                                val_data_loader, epoch,\n",
    "                                                                                save_to=save_model)\n",
    "\n",
    "        if val_score[0] > best_test_acc:\n",
    "            best_test_acc = val_score[0]\n",
    "            best_epoch_num = epoch\n",
    "\n",
    "            torch.save(model.state_dict(), os.path.join('checkpoints', subset, 'gcn_epoch={}_val_acc={}.pth'.format(\n",
    "                best_epoch_num, best_test_acc)))\n",
    "\n",
    "    utils.plot_curves()\n",
    "\n",
    "    class_names = train_dataset.label_encoder.classes_\n",
    "    utils.plot_confusion_matrix(train_gts, train_preds, classes=class_names, normalize=False,\n",
    "                                save_to='output/train-conf-mat')\n",
    "    utils.plot_confusion_matrix(val_gts, val_preds, classes=class_names, normalize=False, save_to='output/val-conf-mat')\n",
    "\n",
    "\n",
    "def train(frequency, model, train_loader, optimizer, epoch):\n",
    "    # set model as training mode\n",
    "    losses = []\n",
    "    scores = []\n",
    "    train_labels = []\n",
    "    train_preds = []\n",
    "\n",
    "    train_count = 0  # counting total trained sample in one epoch\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        X, y, video_ids = data\n",
    "        # distribute data to device\n",
    "        X, y = X.cuda(), y.cuda().view(-1, )\n",
    "\n",
    "        train_count += X.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(X)  # output has dim = (batch, number of classes)\n",
    "\n",
    "        loss = F.cross_entropy(out, y)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # to compute accuracy\n",
    "        y_pred = torch.max(out, 1)[1]  # y_pred != output\n",
    "\n",
    "        step_score = accuracy_score(y.cpu().data.squeeze().numpy(), y_pred.cpu().data.squeeze().numpy())\n",
    "\n",
    "        # collect prediction labels\n",
    "        train_labels.extend(y.cpu().data.squeeze().tolist())\n",
    "        train_preds.extend(y_pred.cpu().data.squeeze().tolist())\n",
    "\n",
    "        scores.append(step_score)  # computed on CPU\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # show information\n",
    "        if (batch_idx + 1) % frequency == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accu: {:.6f}%'.format(\n",
    "                epoch + 1, train_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item(),\n",
    "                100 * step_score))\n",
    "\n",
    "    return losses, scores, train_labels, train_preds\n",
    "\n",
    "\n",
    "def validation(model, test_loader, epoch, save_to):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    val_loss = []\n",
    "    labels = []\n",
    "    labels_pred = []\n",
    "    all_video_ids = []\n",
    "    all_pool_out = []\n",
    "\n",
    "    num_copies = 4\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(test_loader):\n",
    "            # distribute data to device\n",
    "            X, y, video_ids = data\n",
    "            X, y = X.cuda(), y.cuda().view(-1, )\n",
    "\n",
    "            all_output = []\n",
    "\n",
    "            stride = X.size()[2] // num_copies\n",
    "\n",
    "            for i in range(num_copies):\n",
    "                X_slice = X[:, :, i * stride: (i+1) * stride]\n",
    "                output = model(X_slice)\n",
    "                all_output.append(output)\n",
    "\n",
    "            all_output = torch.stack(all_output, dim=1)\n",
    "            output = torch.mean(all_output, dim=1)\n",
    "\n",
    "           \n",
    "            loss = F.cross_entropy(output, y)\n",
    "\n",
    "            val_loss.append(loss.item())  # sum up batch loss\n",
    "            y_pred = output.max(1, keepdim=True)[1]  # (y_pred != output) get the index of the max log-probability\n",
    "\n",
    "            # collect all y and y_pred in all batches\n",
    "            labels.extend(y)\n",
    "            labels_pred.extend(y_pred)\n",
    "            all_video_ids.extend(video_ids)\n",
    "            all_pool_out.extend(output)\n",
    "\n",
    "    # this computes the average loss on the BATCH\n",
    "    val_loss = sum(val_loss) / len(val_loss)\n",
    "\n",
    "    # compute accuracy\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "    labels_pred = torch.stack(labels_pred, dim=0).squeeze()\n",
    "    all_pool_out = torch.stack(all_pool_out, dim=0).cpu().data.numpy()\n",
    "\n",
    "    # log down incorrectly labelled instances\n",
    "    incorrect_indices = torch.nonzero(labels - labels_pred).squeeze().data\n",
    "    incorrect_video_ids = [(vid, int(labels_pred[i].data)) for i, vid in enumerate(all_video_ids) if\n",
    "                           i in incorrect_indices]\n",
    "\n",
    "    labels = labels.cpu().data.numpy()\n",
    "    labels_pred = labels_pred.cpu().data.numpy()\n",
    "\n",
    "    # top-k accuracy\n",
    "    top1acc = accuracy_score(labels, labels_pred)\n",
    "    top3acc = compute_top_n_accuracy(labels, all_pool_out, 3)\n",
    "    top5acc = compute_top_n_accuracy(labels, all_pool_out, 5)\n",
    "    top10acc = compute_top_n_accuracy(labels, all_pool_out, 10)\n",
    "    top30acc = compute_top_n_accuracy(labels, all_pool_out, 30)\n",
    "\n",
    "    # show information\n",
    "    print('\\nVal. set ({:d} samples): Average loss: {:.4f}, Accuracy: {:.2f}%\\n'.format(len(labels), val_loss,\n",
    "                                                                                        100 * top1acc))\n",
    "\n",
    "    if save_to:\n",
    "        # save Pytorch models of best record\n",
    "        torch.save(model.state_dict(),\n",
    "                   os.path.join(save_to, 'gcn_epoch{}.pth'.format(epoch + 1)))  # save spatial_encoder\n",
    "        print(\"Epoch {} model saved!\".format(epoch + 1))\n",
    "\n",
    "    return val_loss, [top1acc, top3acc, top5acc, top10acc, top30acc], labels.tolist(), labels_pred.tolist(), incorrect_video_ids\n",
    "\n",
    "\n",
    "\n",
    "def compute_top_n_accuracy(truths, preds, n):\n",
    "    best_n = np.argsort(preds, axis=1)[:, -n:]\n",
    "    ts = truths\n",
    "    successes = 0\n",
    "    for i in range(ts.shape[0]):\n",
    "        if ts[i] in best_n[i, :]:\n",
    "            successes += 1\n",
    "    return float(successes) / ts.shape[0]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    direc = '/home/jovyan/Documents/DL/DL_Project/WLASL'\n",
    "    subset = 'asl100'\n",
    "    split_file = os.path.join(direc, 'data/splits/{}.json'.format(subset))\n",
    "    pose_data = os.path.join(direc, 'data/pose_per_individual_videos')\n",
    "    config_file = os.path.join(direc, 'code/TGCN/configs/{}.ini'.format(subset))\n",
    "    configs = Config(config_file)\n",
    "    run(file_split=split_file, configs=configs, pose_directory=pose_data)"
   ]
  }
 ]
}